---
title: pythonìœ¼ë¡œ í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ì„ í˜•íšŒê·€ë¶„ì„ ì´ìƒì¹˜, ê²°ì¹˜ ì²˜ë¦¬ ê·¸ë¦¬ê³  ì •ê·œí™”
date: 2021-09-01 16:57:00 +0900
categories: [python, regression]
tags: [python, differentiation, derivative, ë¯¸ë¶„, íŒŒì´ì¬, regression, íšŒê·€ë¶„ì„, ì´ìƒì¹˜, ê²°ì¹˜, ML, ë¨¸ì‹ ëŸ¬ë‹] 
math: true
comments: true
typora-root-url: ../
---

---

## Linear Regression Data Handling

ì´ë²ˆ í¬ìŠ¤íŒ…ì€ **íŒŒì´ì¬**ê³¼ **ê²½ì‚¬í•˜ê°•ë²• (Gradient Descent Algorithm)**ì„ ì´ìš©í•˜ì—¬ ì§€ë‚œë²ˆë³´ë‹¤ ë” ì •í™•í•œ íšŒê·€ë¶„ì„ì„ í•´ë³´ë ¤ê³  í•œë‹¤. ê·¸ë¦¬ê³  ìµœì¢…ì ìœ¼ë¡œ ë‚´ê°€ ì§  ì½”ë“œì™€ ì•Œê³ ë¦¬ì¦˜ì´ `scikit learn` íŒ¨í‚¤ì§€ì—ì„œ ì œê³µí•œ `linear_model`ì— ëª¨ë“ˆì— ë¹„í•´ì„œ ì–¼ë§ˆë‚˜ ì •í™•í•œì§€ ë¹„êµí•´ë³´ë ¤ê³  í•œë‹¤.

**"êµ³ì´ ì¢‹ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë†”ë‘ê³  ì™œ íŒŒì´ì¬ìœ¼ë¡œ, ë¡œìš°ë ˆë²¨ ìŠ¤íƒ€ì¼ë¡œ ì§ì ‘ ì½”ë”©ì„ í•´ì•¼í•˜ëŠëƒ?"** ë¼ê³  ì˜ë¬¸ì„ ê°€ì§ˆ ìˆ˜ë„ ìˆë‹¤. 

ì²«ì§¸ë¡œ, ì§ì ‘ **A to Z** ë™ì‘ì›ë¦¬ë¥¼ íŒŒì•…í•˜ê³  êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì—¬ ì‚¬ìš©í•œë‹¤ë©´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•œ ê¹Šì€ ì´í•´ë„ë¥¼ ê°–ê³  ì˜í™œìš©í•  ìˆ˜ ìˆìœ¼ë¦¬ë¼ ìƒê°í•˜ê¸° ë•Œë¬¸ì´ë©°

ë‘˜ì§¸ë¡œ, ì–´ë–¤ ìœ í˜•ì˜ ìë£Œêµ¬ì¡°ë‚˜ ë³µì¡í•œ ìƒí™© ë“±ì— ë”°ë¼ì„œ`sklearn`ì‚¬ìš©ì— í•œê³„ê°€ ìˆì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. 

ë§ˆì§€ë§‰ìœ¼ë¡œ, (ì´ê²Œ ì œì¼ ì¤‘ìš”í•¨) ê²°êµ­ `TensorFlow`ë¡œ êµ¬í˜„í•  ë•Œ, `GradientDescentOptimizer()`ë¼ëŠ” ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ëŠ”ë°, ì´ë•Œ **ê²½ì‚¬í•˜ê°•ë²• (Gradient Descent Algorithm)**ì„ ì´í•´í•˜ì§€ ëª»í•œì±„ë¡œ ì‚¬ìš©í•œë‹¤ë©´ ë¬´ìŠ¨ ì˜ë¯¸ê°€ ìˆê² ëŠ”ê°€.. ì•”ê¸°ì‹ êµìœ¡ì— ì§€ë‚˜ì§€ ì•ŠëŠ”ë‹¤. 

ìš°ë¦¬ê°€ ë¯¸ì ë¶„ì„ í•  ë•Œ, ê³µì‹ì„ ì™¸ì›Œì„œ í™œìš©í•˜ê¸° ì´ì „ì—, ì–´ë–»ê²Œ ê·¹í•œìœ¼ë¡œë¶€í„° ë¯¸ë¶„ê°’ì´ ë„ì¶œë˜ëŠ”ì§€ ì¦ëª…ì„ í•´ë³´ë©° ì›ë¦¬ë¥¼ ì´í•´í•˜ëŠ” ê³¼ì •ì„ ê²ªëŠ”ë‹¤ë©´ í›¨ì”¬ ë¯¸ë¶„ì— ëŒ€í•œ í™œìš©í­ì´ ë„“ì–´ì§€ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•œ ê³¼ì •ì´ë¼ ìƒê°í•œë‹¤.

ì„œë‘ê°€ ê¸¸ì—ˆë‹¤. ë°ì´í„°ëŠ” `ozone.csv` ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì˜€ëŠ”ë° ë°ì´í„° íŒŒì¼ì€ ê¹ƒí—™ì— ë‚¨ê²¨ë‘ê² ë‹¤! 

í™˜ê²½ë¶€í„° ì„¤ì •í•˜ì.

```python
import numpy as np  # ndarrayê°€ ê¸°ë³¸
import pandas as pd  # data ì²˜ë¦¬ ë° loading
import matplotlib.pyplot as plt  # scatter ì°ê³ , scikit learnì´ êµ¬í˜„í•œê²Œ ë¹„ìŠ·í•œì§€ í™•ì¸ìš©
from sklearn import linear_model
from scipy import stats   # z scoreê°’ì„ ì´ìš©í•œ ì •ê·œí™” (normalisation) ì‘ì—…ì— ì‚¬ìš© 
from sklearn.preprocessing import MinMaxScaler # min max scaling ì‚¬ìš©ì„ í• ê±´ë° ì´ê²ƒ ë˜í•œ sklearnì˜ ë„ì›€ì„ ë°›ì„ê²ƒì„.
```

```python
# ìˆ˜ì¹˜ë¯¸ë¶„ í•¨ìˆ˜. ì—†ë‹¤ë©´ ë³µì‚¬í•´ì„œ ì‚¬ìš©í•˜ê¸¸ ë°”ëŒ
def numerical_derivative(f, x):
    """
    í¸ë¯¸ë¶„ì„ ì—¬ëŸ¬ë²ˆ ëŒë ¤ì•¼í•¨ (vector of partial derivatives)
    f: loss functionì— ëŒ€í•´ì„œ ëŒë¦¼
    x: loss functionì—ì„œ ì‚¬ìš©ë  1ì°¨ì› ì´ìƒì˜ ë°°ì—´ ë°ì´í„°
    """
    delta_x = 1e-4   # x ë³€í™”ëŸ‰ì˜ í¬ê¸° (ê·¹í•œ ëŒ€ì‹  ì•„ì£¼ ì‘ì€ ìˆ˜ë¡œ ëŒ€ì²´)
    derivative_x = np.zeros_like(x)  # xì™€ ë˜‘ê°™ì€ shapeì´ë©° ìš”ì†Œë¥¼ 0ìœ¼ë¡œ ê°–ê³  ìˆëŠ” ë°°ì—´(or í–‰ë ¬) ìƒì„± 
    
    it = np.nditer(x, flags=['multi_index']) # iterator í•˜ë‚˜ ìƒì„±. í”Œë˜ê·¸ëŠ” ë©€í‹° ì¸ë±ìŠ¤ ì„¤ì •
    
    while not it.finished:
        idx = it.multi_index
        tmp = x[idx]   # ì„ì‹œ ë³€ìˆ˜ë¡œ x[idx] ì˜ ì›ê°’ì„ ì €ì¥
        
        x[idx] = tmp + delta_x  # ì „í–¥ ì°¨ë¶„
        fx_plus_delta_x = f(x)  # ì²«ë²ˆì§¸ ì¸ìë¡œ ë“¤ì–´ì˜¨ í•¨ìˆ˜ë¥¼ ëŒ€ì…
        
        x[idx] = tmp - delta_x  # í›„í–¥ ì°¨ë¶„
        fx_minus_delta_x = f(x)  # ì²«ë²ˆì§¸ ì¸ìë¡œ ë“¤ì–´ì˜¨ í•¨ìˆ˜ë¥¼ ë‹¤ì‹œ ëŒ€ì…
        
        # ì¤‘ì•™ ì°¨ë¶„
        derivative_x[idx] = (fx_plus_delta_x - fx_minus_delta_x) / (2 * delta_x)
        x[idx] = tmp # xê°’ ì›ìƒíƒœë¡œ ë³µêµ¬
        it.iternext()  # ë‹¤ìŒ ì¸ë±ìŠ¤ë¡œ ë„˜ì–´ê°€ì
        
    return derivative_x
```

## ë°ì´í„° ë¡œë”©

```python
# Raw Data Loading

df = pd.read_csv('./data/ozone/ozone.csv', sep=',')  # ê²½ë¡œëŠ” ë³¸ì¸ì˜ í™˜ê²½ì— ë§ì¶° ì‚¬ìš©í•˜ì.
display(df.head())  # ë°ì´í„° í•œë²ˆ ì°ì–´ì£¼ê³ .

training_data_set = df[['Temp', 'Ozone']]  # ìš°ì„  ë‹¨ìˆœ ì„ í˜• íšŒê·€, ë…ë¦½ë³€ìˆ˜ë¥¼ í•˜ë‚˜ë§Œ ê°–ì. (temp)
```

## 1. ê²°ì¹˜ê°’ ì²˜ë¦¬

`NaN` ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆë‹¤. 

- ê°’ì„ ëŒ€ì²´ (ëŒ€ì²´ì‹œ ì„ì˜ì˜ ìˆ«ìê°€ ì•„ë‹Œ ë…¼ë¦¬ì— ì˜ê±°í•´ì„œ ê°’ì„ ê²°ì •)
- ê°’ì„ ì‚­ì œ

ê²½ìš°ì— ë”°ë¼ ì´ìƒì ì¸ ë°©ì•ˆì´ ìˆê¸°ëŠ” í•˜ë‚˜, ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„  ê°’ì„ ì‚­ì œí•˜ê³  ê°€ì. 

```python
training_data = training_data_set.dropna(how='any')   # .dropna ë©”ì†Œë“œë¥¼ ì‚¬ìš©
display(training_data.shape) # (116, 2)   
```

## 2. ì´ìƒì¹˜(outliers) ì²˜ë¦¬ 

```python
# ì´ìƒì¹˜ ì²˜ë¦¬
# ì˜¨ë„ì— ëŒ€í•´ì„œë„ ì´ìƒí•œ ê°’ì´ ìˆëŠ”ì§€ íŒë‹¨í›„ ì²˜ë¦¬ (ì§€ëŒ€ì  ì²˜ë¦¬)
# ì˜¤ì¡´ê°’ì—ì„œë„ ì´ìƒí•œ ê°’ì´ ìˆìœ¼ë©´ íŒë‹¨í›„ ì²˜ë¦¬ (outlier)

# ì•„ì›ƒë¼ì´ì–´ ì²˜ë¦¬ì‹œ í•­ìƒ ì¡°ì‹¬í•´ì•¼í•¨. 
# ì˜¨ë„ì— ëŒ€í•œ ì´ìƒì¹˜ë¥¼ ë‚ ë¦¬ê³ , ì´í›„ì— ì˜¤ì¡´ì— ëŒ€í•´ ë‚ ë¦°ë‹¤ê³  í•˜ì.
# ê·¼ë° ì²« ì´ìƒì¹˜ë¥¼ ë–¼ì–´ë‚´ë©´, í•˜ë‚˜ê°€ ë–¼ì–´ì§„ ìƒíƒœì—ì„œ ë‹¤ì‹œ ì´ìƒì¹˜ë¥¼ í™•ì¸í•˜ë©´ ì•„ì›ƒë¼ì´ì–´ê°€ ë°”ë€”ìˆ˜ ìˆìŒ. 
# ë‚¨ì€ ë°ì´í„°ë¡œ ì²˜ë¦¬ë¥¼ í•˜ë©´ ê¸°ì¤€ ìì²´ê°€ ë°”ë€ë‹¤.

# Tempì— ëŒ€í•œ ì´ìƒì¹˜(ì§€ëŒ€ì ) ì²˜ë¦¬
zscore_threshold = 1.8  # zscore ê°’ì„ ì¡ì•˜ë‹¤. ë³´í†µ 1.96ì¼ë•Œ ì•½ 97%ì˜ ë°ì´í„°ë¥¼ í¬í•¨í•œë‹¤. 
                        # ì—¬ê¸°ì„  1.8ë¡œ ì¡°ê¸ˆì€ ëŠìŠ¨í•˜ê²Œ ì¡ì•˜ë‹¤

# ì˜¨ë„ì— ìˆëŠ” ì§€ëŒ€ê°’ì„ ì¶”ì¶œ
np.abs(stats.zscore(training_data['Temp']))  # ì ˆëŒ€ê°’ìœ¼ë¡œ ë³€í™˜. zscore thresholdë‘ ë¹„êµí•´ì•¼í•¨
np.abs(stats.zscore(training_data['Temp'])) > zscore_threshold   # boolean mask ê°€ ë‚˜ì˜´

# Falseë¡œ ë‚˜ì˜¤ë©´ ì§€ëŒ€ê°’ì´ ì•„ë‹ˆë€ ì†Œë¦¬. Trueë¼ë©´ ì•„ì›ƒë¼ì´ì–´, ì§€ëŒ€ê°’, ì´ìƒì¹˜ë€ ì†Œë¦¬. (z scoreë¥¼ ê¸°ì¤€ìœ¼ë¡œ í–ˆì„ë•Œ)

outlier = training_data['Temp'][ np.abs(stats.zscore(training_data['Temp'])) > zscore_threshold ]  # True ë¶€ë¶„ë§Œ ë½‘ì•„ëƒ„.
print(outlier) # ì§€ëŒ€ì 

# training_data['Temp'].isin(outlier)  # Falseì˜ ì˜ë¯¸ê°€ ì§€ëŒ€ì ì´ ì•„ë‹ˆë€ ì†Œë¦¬ì„. 
~training_data['Temp'].isin(outlier) # ì•ì— tildeë¥¼ ì£¼ë©´ì„œ ë…¼ë¦¬ê°’ì´ ë°˜ëŒ€ë¡œ ë°”ë€œ.

training_data['Temp'][ ~training_data['Temp'].isin(outlier) ]  # 110ê°œì˜ ë°ì´í„°ë§Œ ë‚¨ìŒ

training_data = training_data.loc[ ~training_data['Temp'].isin(outlier), :]  # 110 x 2 
```

ì´ë²ˆì—” `t`(ë ˆì´ë¸”)ì˜ ê°’ì„ ê°–ê³  ìˆëŠ” '**Ozone**'ì— ëŒ€í•´ì„œ ì´ìƒì¹˜ë¥¼ ì²˜ë¦¬í•´ë³´ì.

```python
# ë§ˆì°¬ê°€ì§€ë¡œ outlier ì„¤ì •
outlier = training_data['Ozone'][ np.abs(stats.zscore(training_data['Ozone'])) > zscore_threshold ]  

training_data = training_data.loc[ ~training_data['Ozone'].isin(outlier), :] # 103 rows Ã— 2 columns
```

ì—¬ê¸°ê¹Œì§€ ì™„ë£Œí–ˆë‹¤ë©´ 103ê°œì˜ caseì™€ 2ê°œì˜ ì—´ì„ ê°€ì§„ `DataFrame`ìœ¼ë¡œ ì •ì œëœë‹¤.

## 3. ì •ê·œí™” (Normalization)

zscoreë¥¼ í†µí•œ ì •ê·œí™”, min-max scaling ë“± ì—¬ëŸ¬ ë°©ë²•ì´ ìˆì§€ë§Œ ì´ê±°ì— ëŒ€í•œ ì²¨ì–¸ ë° ë¶€ì—°ì„¤ëª…ì€ ë‹¤ë¥¸ í¬ìŠ¤íŒ…ì—ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ì. 

ì—¬ê¸°ì„œëŠ” **Min-Max Normalization**ì„ ì‚¬ìš©í•œë‹¤.

ë§¨ì²˜ìŒ í™˜ê²½ì„ ì„¸íŒ…í•  ë•Œ ë¶ˆëŸ¬ì™”ë˜ `sklearn`íŒ¨í‚¤ì§€ì˜ `preprocessing`ëª¨ë“ˆì˜ `MinMaxScaler` í´ë˜ìŠ¤ë¥¼ ì´ìš©í•œë‹¤. 

```python
# ì…ë ¥ê°’ì— ëŒ€í•œ ìŠ¤ì¼€ì¼ë§ì„ í•˜ëŠ” ê°ì²´ë¥¼ ë§Œë“¤ê²ƒì„
scaler_x = MinMaxScaler() # scaling ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ê°ì²´ë¥¼ í•˜ë‚˜ ìƒì„± (ì…ë ¥ê°’ ì²˜ë¦¬ í•˜ëŠ”ë†ˆ 1ê°œ, label ì²˜ë¦¬ í•˜ëŠ” ë†ˆ 1ê°œ)
scaler_y = MinMaxScaler()

scaler_x.fit(training_data['Temp'].values.reshape(-1,1))  
scaler_y.fit(training_data['Ozone'].values.reshape(-1,1))
# fitting ì‘ì—…ì„ í•¨. ë³€í™˜ì‹œí‚¤ëŠ” ì‘ì—…ì€ ì•„ë‹˜. 
# ìµœì†Œ, ìµœëŒ“ê°’ì„ ì•Œê¸°ìœ„í•œ ì‘ì—….
# fitì•ˆì—ëŠ” 2ì°¨ì› ndarrayë¡œë§Œ ë‚˜ì™€ì•¼í•¨
# dataì˜ ê°œìˆ˜, ìµœëŒ€, ìµœì†Œê°’ë“¤ì„ scalerì— ì €ì¥!

# ë°‘ì—ëŠ” dfì•ˆì— ìˆëŠ” ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ì˜ ìŠ¤ì¼€ì¼ì„ ì •ê·œí™”í•˜ëŠ” ì½”ë“œì„. 
training_data['Temp'] = scaler_x.transform(training_data['Temp'].values.reshape(-1,1))
training_data['Ozone'] = scaler_y.transform(training_data['Ozone'].values.reshape(-1,1))

```

( "**transformì„ í•˜ëŠ”ë° ì™œ reshapeì„ í•˜ëŠ”ê°€?**" ì˜ë¬¸ì´ ë“ ë‹¤ë©´, ì„ í˜•ëŒ€ìˆ˜í•™ì—ì„œ ë§¤íŠ¸ë¦­ìŠ¤ íŠ¸ëœìŠ¤í¬ì¦ˆ í•  ë•Œ í–‰ë ¬ í˜•íƒœì—ì„œë§Œ ê°€ëŠ¥í•˜ë‹¤ê³  ì¹˜ë¶€í•˜ë©´ ì´í•´í•˜ê¸°ê°€ ì‰½ë‹¤.)

ì´ì œ **training data set**ì— ëŒ€í•œ ì¤€ë¹„ê°€ ëë‚¬ë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ëŒë¦´ì¼ë§Œ ë‚¨ì•˜ë‹¨ ì†Œë¦¬ë‹¤. ğŸ˜‰ 

## 4. Training Data Set

```python
x_data = training_data['Temp'].values.reshape(-1, 1)
t_data = training_data['Ozone'].values.reshape(-1, 1)
```



## 5. Wì™€ b ì„¤ì • (W: weight (ê°€ì¤‘ì¹˜) & b: bias (í¸í–¥))

Wì™€ bëŠ” ë‚œìˆ˜ë¡œ ì¡ì•„ë‘”ë‹¤. 

ë‹¨, Wê°™ì€ ê²½ìš°ëŠ” ë…ë¦½ë³€ìˆ˜ì™€ ë§¤íŠ¸ë¦­ìŠ¤ ê³±í•˜ê¸° ì—°ì‚°ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì—, "**1x1**" í–‰ë ¬ë¡œ ë³€í™˜ì‹œì¼œì„œ ë‚œìˆ˜ë¥¼ ë°›ëŠ”ë‹¤.

```python
# Wì™€ b ë¥¼ ì •ì˜ (Weight & bias)
W = np.random.rand(1,1)   # 1x1  (2ì°¨ì›)
b = np.random.rand(1)   # 1ì°¨ì› ë°±í„°
```



## 6. ì˜ˆì¸¡ í•¨ìˆ˜ì™€ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜, ê·¸ë¦¬ê³  Learning rate ì„¤ì •

```python
# ìš°ë¦¬ê°€ ë§Œë“  íŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„í•œ ì˜ˆì¸¡í•¨ìˆ˜ í•˜ë‚˜ ì •ì˜
def predict(x):
    return np.dot(x, W) + b

# Wì™€ bë¥¼ êµ¬í•˜ë ¤ë©´? loss functionì´ í•„ìš”í•˜ë‹¤. ë˜ ìˆ˜ì¹˜ë¯¸ë¶„ í•¨ìˆ˜ê°€ í•„ìš”í•˜ë‹¤.
def loss_func(input_obj):   # input_objëŠ” [Wì˜ ê°’, bì˜ ê°’]  1ì°¨ì› ndarrayë¡œ ë°›ìŒ
    
    input_W = input_obj[0].reshape(1, 1)  # 2ì°¨ì› ë§¤íŠ¸ë¦­ìŠ¤
    input_b = input_obj[1]
    
    # í‰ê· ì œê³±ì˜¤ì°¨ë¥¼ êµ¬í•´ì•¼í•¨ => loss í•¨ìˆ˜ì˜ ê°’. ì˜ˆì¸¡ì¹˜ë¥¼ ì•Œì•„ì•¼í•¨!
    
    y = np.dot(x_data, input_W) + input_b  # ì…ë ¥ê°’ì— ëŒ€í•´ í˜„ì¬ Wì™€ bë¥¼ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡ì¹˜ ê³„ì‚°
    
    return np.mean(np.power((t_data - y), 2))   # [ì‹¤ì œ ê°’(t) - ì˜ˆì¸¡ê°’(y)] ì˜ ì œê³±.

# learning rate ìƒìˆ˜ ì„¤ì •
learning_rate = 1e-4
```

ìœ„ ì½”ë“œë¥¼ ì½ë‹¤ë³´ë©´ "**ì—¥ ìˆ˜ì¹˜ë¯¸ë¶„ í•¨ìˆ˜ëŠ” ì–´ë””ê°”ì§€?**" ì˜ë¬¸ì´ ë“¤ìˆ˜ë„ ìˆëŠ”ë°, ì´ˆê¸° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ë©´ì„œ ì„¸íŒ…í• ë•Œ ê°™ì´ ë¹¼ë‘ì—ˆë‹¤.



## 7. ë°˜ë³µí•™ìŠµ ê³ ê³  âš™ï¸ 

```python
# ì´ì œ ì¤€ë¹„ëœ ë‚´ìš©ì„ ê°–ê³  ë°˜ë³µí•™ìŠµì„ ì§„í–‰í•˜ì!
for step in range(300000):
    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)  # Wì™€ bë¥¼ 1ì°¨ì›ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ì—°ê²°ì‹œí‚¨ë‹¤.
    
    result_derivative = learning_rate * numerical_derivative(loss_func, input_param)
    
    W = W - result_derivative[0].reshape(-1, 1)
    b = b - result_derivative[1]
    
    if step % 30000 == 0:
        print('loss: {}'.format(loss_func(input_param)))  # loss ê°’ì´ ì‘ì•„ì•¼ ì¢‹ì€ê²ƒì„. 0ì— ê°€ê¹Œì›Œì•¼ í›Œë¥­ìŠ¤.
    
# lossê°’ì´ í¬ë‹¤ë©´, learning rate ìˆ˜ì •ê³¼ ë°˜ë³µíšŸìˆ˜ ì¦ê°€ ë“±ì„ í†µí•´ ì¬ì¡°ì •ì„ í•´ë³´ì.
```

ì•„ë˜ì— ê²°ê³¼ê°’ì€ ë‚´ê°€ ì§ì ‘ ì½”ë“œë¥¼ ëŒë ¸ì„ ë•Œ ì–»ì€ loss functionì˜ ê°’ì´ë‹¤. ë„ˆë¬´ í¬ê³  ë§Œì¡±ìŠ¤ëŸ½ì§€ ëª»í•˜ë‹¤. ë°ì´í„° ì •ì œ, ë°˜ë³µ íšŸìˆ˜, ì„ì˜ë¡œ ì„¤ì •í•œ learning rate ë“±ë“± ì—¬ëŸ¬ ìš”ì¸ì´ ìˆê¸°ë•Œë¬¸ì— ì™„ë²½í•  ìˆ˜ëŠ” ì—†ë‹¤.

```python
loss: 1.014186118583775
loss: 0.039834601720002256
loss: 0.03539345409514608
loss: 0.03302686573813619
loss: 0.03176569847265413
loss: 0.031093615866125395
loss: 0.030735459543517578
loss: 0.030544596173185046
loss: 0.0304428840887258
loss: 0.030388681188991663
```



## 8. ì˜ˆì¸¡ì„ í•´ë³´ì

```python
# ê°’ì€ 2ì°¨ì› í–‰ë ¬ë¡œ ì£¼ì–´ì•¼í•¨.
# í•¨ìˆ˜ëŠ” ì•„ê¹Œ ì •ì˜í•´ë†“ì•˜ë‹¤.
# Temp ê°’ì„ í™”ì”¨ 62ë„ë¡œ ì¤˜ë³´ì.  
# ì •ê·œí™” í–ˆê¸° ë•Œë¬¸ì— ì•„ë˜ ë³€í™˜ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.
scaled_predict_data = scaler_x.transform([[62]])   
scaled_result = scaler_y.inverse_transform(predict(scaled_predict_data)) 
# ì •ê·œí™”ëœ ê°’ì„ ë„£ì–´ ì˜ˆì¸¡ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í–ˆë‹¤ë©´, ë‹¤ì‹œ ì •ê·œí™” ì´ì „ ìƒíƒœì˜ ê°’ìœ¼ë¡œ ëŒë ¤ì„œ ë‚˜íƒ€ëƒ„. ì™œ? ì½ê¸° ì‰¬ìš°ë‹ˆê¹
print(scaled_result)
```

ê²°ê³¼ê°’:

```python
[[2.6140319]]
```

---

ë¨¸ì‹ ëŸ¬ë‹.. ì´ë¡ ë¿ë§Œì•„ë‹ˆê³  êµ¬í˜„ë„ ì´ë ‡ê²Œ ë³µì¡í•œ ê±°ì˜€ë‹ˆ?

ìœ„ì—ì„œë„ ì–¸ê¸‰í–ˆì§€ë§Œ, íŒ¨í‚¤ì§€ë¥¼ ì“°ë©´ í›¨ì”¬ ë” ë¹ ë¥´ê³ , ì‰½ê³ , ì •í™•í•˜ê²Œ ê°’ì„ ë„ì¶œí•´ë‚¼ ìˆ˜ ìˆë‹¤. 

ê°™ì€ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ `sklearn`ì„ í™œìš©í•´ë³´ê³ , íŒŒì´ì¬ì˜ ì˜ˆì¸¡ ê°’ê³¼ ë¹„êµí•´ë³´ì! 

## SciKit Learnì„ ì´ìš©í•œ ì˜ˆì¸¡

```python
# sklearn íŒ¨í‚¤ì§€ ì•ˆì— ìˆëŠ” linear_model ëª¨ë“ˆì—ì„œ LinearRegression í´ë˜ìŠ¤ë¥¼ ì´ìš©
model = linear_model.LinearRegression() 

model.fit(x_data, t_data)   # ë°ì´í„°ëŠ” ì•„ê¹Œì™€ ê°™ìŒ. fitì„ ì´ìš©í•´ í›ˆë ¨ì„ ì‹œí‚¤ì.
scaled_predict_data = scaler_x.transform([[62]])   # ì˜ˆì¸¡ì„ ìœ„í•´ í™”ì”¨ 62ë„ë¥¼ ë„˜ê²¨ì£¼ì—ˆìŒ
scaled_result = model.predict(scaled_predict_data) # ì˜ˆì¸¡ëœ ê°’ì„ ë‹¤ì‹œ ì •ê·œí™” -> ì›ë³¸ í˜•íƒœë¡œ ë°”ê¿ˆ
print(scaler_y.inverse_transform(scaled_result))  
```

ê²°ê³¼ê°’:

```python
[[1.75864872]]
```



- íŒŒì´ì¬ì—ì„œ ë¡œìš°ë ˆë²¨ ìŠ¤íƒ€ì¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ê²°ê³¼ ê°’: **2.6140319**
- `sklearn`ìœ¼ë¡œ í•œ regressionì—ì„œì˜ ê²°ê³¼ ê°’: **1.75864872**

ì°¨ì´ê°€ ìƒë‹¹íˆ ë§ì´ë‚œë‹¤. ë¬¼ë¡  `sklearn` ì´ í›¨ì”¬ ì •í™•í•˜ë‹¤.



ì™œ ì°¨ì´ê°€ ë‚œê±¸ê¹Œ?  ì•ì„œë„ ë§í–ˆì§€ë§Œ, íŒŒì´ì¬ì„ ì´ìš©í•œ ë¡œìš°ë ˆë²¨ ìŠ¤íƒ€ì¼ ì½”ë”©ì—ì„  ìƒìˆ˜ê°’ì„ ì •í•´ì£¼ê³ , ë°˜ë³µê°’ì„ ì§ì ‘ ì •í•´ì¤˜ì•¼í•˜ëŠ” ìˆœê°„ì´ ìˆë‹¤.

ì—¬ê¸°ì„œ ëŒ€ë¶€ë¶„ ì°¨ì´ê°€ ë‚œë‹¤ê³  ìƒê°ì„ í•œë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ ì–´ë–»ê²Œ, ê·¸ë¦¬ê³  ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ë¥¼ ëˆˆìœ¼ë¡œ í™•ì¸í•´ë³´ì.

```python
fig = plt.figure()

fig_python = fig.add_subplot(1, 2, 1)  # 1í–‰ 2ì—´ ê·¸ì¤‘ì—ì„œ ì²«ë²ˆì§¸ (1)
fig_sklearn = fig.add_subplot(1, 2, 2)  # 1í–‰ 2ì—´ ë‘ë²ˆì§¸(2)

fig_python.set_title('Using Python')   # ì²«ë²ˆì§¸ ê·¸ë˜í”„ íƒ€ì´í‹€ 
fig_sklearn.set_title('Using sklearn') # ë‘ë²ˆì§¸ ê·¸ë˜í”„ íƒ€ì´í‹€

fig_python.scatter(x=x_data, y=t_data)
fig_python.plot(x_data, x_data*W.ravel()+b, color='r')  # ì²«ë²ˆì§¸ ê·¸ë˜í”„ ì‚°ì ë„ì— ëŒ€í•œ íŠ¸ë Œë“œ ë¼ì¸

# ë§ˆì°¬ê°€ì§€ë¡œ ë‘ë²ˆì¬ ê·¸ë˜í”„.
fig_sklearn.scatter(training_data['Temp'].values, 
                    training_data['Ozone'].values)
fig_sklearn.plot(training_data['Temp'].values, 
                 training_data['Temp'].values*model.coef_.ravel() + model.intercept_, color='g')

fig.tight_layout()
plt.show()
```

![scatter](/../assets/images/regression/scatter.png)

ì´ë ‡ê²Œ ì¶”ì„¸ì„ ì—ì„œ ì°¨ì´ê°€ ë‚˜ëŠ”ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. 

ì•„ ì°¸ê³ ë¡œ ì‚°ì ë„ ëª¨ì–‘ì´ ì¡°ê¸ˆ ë‹¤ë¥¸ ì´ìœ ëŠ” `python`ìœ¼ë¡œ ê·¸ë ¸ì„ ë•ŒëŠ” ì •ê·œí™”ë¥¼ í•´ë†“ì€ ìƒíƒœì˜€ê³ , ì§€ê¸ˆì€ `sklearn`ì—ì„œëŠ” ì•„ë‹ˆë‹¤ëŠ” ì ì—ì„œ ë‚˜ì˜¤ëŠ” ì°¨ì´ì¸ë°.. ì´ê±´ í•„ìê°€ ê·€ì°®ì•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì •ê·œí™”ë¥¼ í•˜ê³  ë§¨ìœ„ì— ìˆëŠ” outlierë¥¼ ì œì™¸í•´ë³´ì•˜ì§€ë§Œ ìœ ì˜ë¯¸í•œ ë³€í™”ëŠ” ì—†ì—ˆë‹¤.

ì˜¤ëŠ˜ì€ ì—¬ê¸°ê¹Œì§€...

ë„ìíŠ¸. ğŸ‘‹ 