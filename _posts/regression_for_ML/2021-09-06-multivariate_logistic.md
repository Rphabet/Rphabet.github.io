---
title: pythonìœ¼ë¡œ í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ë‹¤ë³€ìˆ˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ 
date: 2021-09-06 15:18:00 +0900
categories: [python, regression]
tags: [python, regression, íšŒê·€ë¶„ì„, ML, ë¨¸ì‹ ëŸ¬ë‹, tensorflow, í…ì„œí”Œë¡œìš°, logistic, ë¡œì§€ìŠ¤í‹±] 
math: true
comments: true
typora-root-url: ../
---

---

# Multivariate Logistic Regression

ì§€ë‚œë²ˆ ë‹¨ì¼ë³€ìˆ˜ë¥¼ ì´ìš©í•œ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ í¬ìŠ¤íŒ…ì„ ì˜¬ë¦° ë’¤, ê·¸ëŒ€ë¡œ ë‹¤ë¥¸ ì˜ì—­ì— ëŒ€í•´ ì†Œê°œí•˜ë ¤ í–ˆìœ¼ë‚˜ ì—°ìŠµê²¸ í•œë²ˆ ë” ë¡œì§€ìŠ¤í‹±ì„ ë‹¤ë£¨ëŠ” ì»¨í…ì¸ ë¥¼ ì˜¬ë¦¬ê¸°ë¡œ ë§ˆìŒì„ ë¨¹ì—ˆë‹¤. 

ì´ë²ˆì—” `python`, `sklearn`, ê·¸ë¦¬ê³  `tensorflow`ë¥¼ ì´ìš©í•´ ê°ê¸° ë‹¤ë¥¸ 3ë²ˆì˜ íšŒê·€ë¶„ì„ì„ í•´ë³´ë„ë¡ í•˜ì. 

ëŒ€í•™ì› ì…í•™ì— ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ë‹¤ë£°ê²ƒì´ë©°, ë ˆì´ë¸”ì€ í•©ê²©/ë¶ˆí•©ê²©ìœ¼ë¡œ í•©ê²©ì¼ ë•Œ 1ì˜ ê°’ì„, ë¶ˆí•©ê²©ì¼ ë•Œ 0ì˜ ê°’ì„ ì·¨í•˜ëŠ” admission ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ì˜ˆì •ì´ë‹¤. 

ë°ì´í„°ëŠ” ì¶”í›„ githubì— ì˜¬ë ¤ë‘ê² ë‹¤.

## í™˜ê²½ì„¤ì •

```python
import numpy as np
import pandas as pd
from scipy import stats # ì´ìƒì¹˜ ì²˜ë¦¬ ì‹œ ì‚¬ìš©ë  zscore
from sklearn import linear_model  # sklearnì˜ logistic regression
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler  # ì •ê·œí™” ë•Œ ì‚¬ìš©ë  min max scaler
```

## Data Preprocessing

### ë°ì´í„° ë¡œë”©. 

```python
df = pd.read_csv('./data/admission/admission.csv', sep=',')
display(df)
```

### ê²°ì¹˜ê°’ ì²˜ë¦¬

```python
training_data = df.dropna(how='any')  # NaNì´ ìˆëŠ” í–‰ ë¬´ì¡°ê±´ ì‚­ì œ
```

### ì´ìƒì¹˜ ì²˜ë¦¬

(ì´ìƒì¹˜ë¥¼ ì²˜ë¦¬ íŒë‹¨ì— ë„ì›€ì„ ì£¼ëŠ” ì‚°ì ë„ë‚˜ ë°•ìŠ¤ ê·¸ë˜í”„ëŠ” ìš°ì„ ì€ ë„˜ì–´ê°€ê² ë‹¤.)

```python
# zscoreë¥¼ ê½¤ë‚˜ ê´€ëŒ€í•˜ê²Œ ì¡ì•„ë³´ì•˜ë‹¤. ì•½ 98%ì˜ ìƒ˜í”Œì„ ì‚¬ìš©.
zscore_threshold = 2.0

# gpaì— ìˆëŠ” ì´ìƒì¹˜ ì²˜ë¦¬
b_mask = np.abs(stats.zscore(df['gpa'])) > zscore_threshold
outlier1 = df['gpa'][ b_mask ]

# greì— ìˆëŠ” ì´ìƒì¹˜ ì²˜ë¦¬
b_mask2 = np.abs(stats.zscore(df['gre'])) > zscore_threshold
outlier2 = df['gre'][ b_mask2 ]

li = []
li.append(outlier1.index.values)
li2 = []
li2.append(outlier2.index.values)
result = []

for idx in li[0]:
    result.append(idx)
for idx in li2[0]:
    result.append(idx)
print(result)
outliers = result

# pandasì˜ ilocì„ ì‚¬ìš©í•˜ì—¬ outlierê°’ì´ ì—†ëŠ” í–‰ë“¤ë¡œë§Œ data frameì„ êµ¬ì„±
clean_data = df.iloc[ ~df.index.isin(outliers) ,:]

display(clean_data)   
```

<img src="/../assets/images/regression/clean_df.png" alt="clean_df" style="zoom:50%;" />

- `admit`: 1ì€ í•©ê²©, 0ì€ ë¶ˆí•©ê²©   (`admit`ì€ ë ˆì´ë¸”ë¡œ ì“°ì„)
- `gre`: ëŒ€í•™ì› ì…í•™ì‹œí—˜ `GRE` ì ìˆ˜
- `gpa`: í•™ë¶€ í•™ì  í‰ê· 
- `rank`: ì§€ì›í•˜ëŠ” í•™êµì˜ ìˆœìœ„

### ì •ê·œí™” (Normalization)

ì •ê·œí™”ëŠ” `python`ì´ë‚˜ `tensorflow`ë¡œ ì‘ì—…í•  ë•Œ í•„ìˆ˜ì ìœ¼ë¡œ ì‚¬ìš©ë¨ì„ ì•Œì•„ë‘ì.

`sklearn`ê°™ì€ ê²½ìš°ì—ëŠ” ìì²´ì ìœ¼ë¡œ ì •ê·œí™”ë¥¼ í•´ì£¼ê¸°ë•Œë¬¸ì— êµ³ì´ ìƒê´€ì´ ì—†ë‹¤.

```python
scaler_x = MinMaxScaler()
scaler_x.fit(clean_data.iloc[:, 1:].values)

new_df = clean_data.iloc[:, :].copy()
new_df.iloc[:, 1:] = scaler_x.transform(new_df.iloc[:, 1:])
```

labelì¸ `admit`ì˜ ê°’ì€ ì–´ì°¨í”¼ 0, 1 ë‘˜ì¤‘í•˜ë‚˜ ë˜ ìŠ¤ì¼€ì¼ ìì²´ê°€ 0~1 ì‚¬ì´ì— ìˆëŠ” ê°’ì„ ì·¨í•˜ê¸°ë•Œë¬¸ì— êµ³ì´ ì •ê·œí™” ì‘ì—…ì„ í•´ì¤„ í•„ìš”ëŠ” ì—†ë‹¤.

### ìµœì¢… ë°ì´í„° ì…‹ ì¤€ë¹„

```python
# ë…ë¦½ë³€ìˆ˜ ë°ì´í„°, ë ˆì´ë¸” ë°ì´í„° ë”°ë¡œ ë¶„ë¦¬
x_data = new_df.iloc[:, 1:]   # n x 3 2ì°¨ì› í–‰ë ¬
t_data = new_df.iloc[:, 0].reshape(-1, 1)   # n x 1  2ì°¨ì› í–‰ë ¬ë¡œ ë³€í™˜

# ê°€ì¤‘ì¹˜ì™€ bias ë‚œìˆ˜ë¡œ ì°¨ì› ë§ì¶°ì„œ ì„¤ì •
W = np.random.rand(3, 1)
b = np.random.rand(1)
```

## 1. Using Python

```python
# ìˆ˜ì¹˜ë¯¸ë¶„ í•¨ìˆ˜ê°€ ìˆì–´ì•¼ í•¨
def numerical_derivative(f, x):

    delta_x = 1e-4   
    derivative_x = np.zeros_like(x)  
    
    it = np.nditer(x, flags=['multi_index'])
    
    while not it.finished:
        idx = it.multi_index
        tmp = x[idx]   
        
        x[idx] = tmp + delta_x 
        fx_plus_delta_x = f(x) 
        
        x[idx] = tmp - delta_x 
        fx_minus_delta_x = f(x)
        
        derivative_x[idx] = (fx_plus_delta_x - fx_minus_delta_x) / (2 * delta_x)
        
        x[idx] = tmp         
        it.iternext() 
        
    return derivative_x
```

```python
# íŒŒì´ì¬ìœ¼ë¡œ ì“¸ loss functionê³¼ ì˜ˆì¸¡ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì.
# logistic ì˜ˆì¸¡í•¨ìˆ˜
def logistic_predict(x):
    z = np.dot(x, W) + b
    y = 1 / (1 + np.exp(-1 * z))
    
    result = 0
    if y < 0.5:
        result = 0
    else: 
        result = 1

    return result, y

# loss function
def loss_func(input_value):
    
    input_w = input_value[:-1].reshape(-1,1)
    input_b = input_value[-1]
    
    z = np.dot(x_data, input_w) + input_b
    
    y = 1 / (1 + np.exp(-1 * z))
    
    delta = 1e-7
    
    return -np.sum( (t_data * np.log(y + delta)) + ((1 - t_data) * np.log(1 - y + delta)) )

# ìƒìˆ˜ ì„¤ì •
learning_rate = 1e-4
```

(ë”°ë¡œ loss í•¨ìˆ˜ì™€ logisticì˜ sigmoidì— ëŒ€í•´ì„œ ìˆ˜ì‹ì ì¸ ë¶€ë¶„ì€ ì ì§€ ì•Šì•˜ë‹¤. ê·¸ê±´ ì „ í¬ìŠ¤íŒ… [ë¡œì§€ìŠ¤í‹± íšŒê·€](https://rphabet.github.io/posts/python_logistic_regression/)ë¥¼ ì°¸ê³ í•˜ì.)

```python
# ë°˜ë³µí•™ìŠµ ê³ ê³ 
for step in range(300000):
    
    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)
    result_derivative = learning_rate * numerical_derivative(loss_func, input_param)

    W = W - result_derivative[:-1].reshape(-1, 1)
    b = b - result_derivative[-1]
    
    if step % 30000 == 0:
        print("lossê°’ì€ {}".format(W, b, loss_func(input_param)))
```

```python
lossê°’ì€ 430.97272711844545
lossê°’ì€ 1.6738163473583776
lossê°’ì€ 0.8337907472538344
lossê°’ì€ 0.5551179253193748
lossê°’ì€ 0.4160607461242618
lossê°’ì€ 0.3327186321044007
lossê°’ì€ 0.27719575662511164
lossê°’ì€ 0.2375552132020219
lossê°’ì€ 0.2078347927438515
lossê°’ì€ 0.18472466508814028
```

ì´ì œ ì˜ˆì¸¡ì„ í•´ë³´ì.

```python
scaled_predict_data = scaler_x.transform([[600, 3.8, 1]])   # ì˜ˆì¸¡ê°’ì— ëŒ€í•´ ì •ê·œí™” ì‹¤ì‹œ          
print(scaled_predict_data)
print(scaler_x.data_max_)

scaled_result = logistic_predict(scaled_predict_data)
print(scaled_result) 
```

ì´ë ‡ê²Œ ë§ˆì§€ë§‰ ì¶œë ¥ë¬¸ì„ ì°ìœ¼ë©´ 0ê³¼ 1ì˜ ë ˆì´ë¸” ê²°ê³¼ê°’ê³¼ logisticí™•ë¥  ê°’ì´ `tuple`ë¡œ ë°˜í™˜ë˜ì–´ ì°íŒë‹¤! 

---

## 2. tensorflow

ì´ë²ˆì—” tensorflowë¡œ ì‹¤í–‰í•´ë³´ì.

```python
# ë°ì´í„° ì…ë ¥ë°›ê¸° ìœ„í•œ placeholder node ì„¤ì •
X = tf.placeholder(shape=[None, 3], dtype=tf.float32)   # ë…ë¦½ë³€ìˆ˜ 3ê°œ ì“°ì„. n x 3 í–‰ë ¬
T = tf.placeholder(shape=[None, 1], dtype=tf.float32)   # ì¢…ì†ë³€ìˆ˜ëŠ” ì–´ì°¨í”¼ í•˜ë‚˜ë°–ì— ì—†ìŒ

# W and b ì„¤ì •
W = tf.Variable(tf.random.normal([3, 1]))  # ë‚œìˆ˜ë¥¼ 3x1 í–‰ë ¬ë¡œ ìƒì„±
b = tf.Variable(tf.random.normal([1])) 

# Hypothesis ë…¸ë“œ ìƒì„±
logit = tf.matmul(X, W) + b
H = tf.sigmoid(logit)

# loss í•¨ìˆ˜ ë…¸ë“œ ìƒì„±
loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))

# í•™ìŠµ train ë…¸ë“œ ìƒì„±
train = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss)

# session ê°ì²´ í•˜ë‚˜ ì¡ì•„ì£¼ì.
sess = tf.Session()
# session ë³€ìˆ˜ ì´ˆê¸°í™”
sess.run(tf.global_variables_initializer())

# ë°˜ë³µí•™ìŠµ ê³ ê³ 
for step in range(300000):
    tmp, loss_val = sess.run([train, loss], feed_dict={X: x_data, T: t_data})
    
    if step % 30000 == 0:
        print('loss is {}'.format(loss_val))
```

```python
loss is 0.8436719179153442
loss is 0.6416785717010498
loss is 0.6002219915390015
loss is 0.5884969830513
loss is 0.5846560597419739
loss is 0.5832125544548035
loss is 0.5826002359390259
loss is 0.582312285900116
loss is 0.5821653604507446
loss is 0.582085371017456
```

ì˜ˆì¸¡ì„ í•´ë³´ì.

```python
# tensorflow prediction

scaled_predict_data = scaler_x.transform([[600, 3.8, 1]])

result = sess.run(H, feed_dict={X:scaled_predict_data})  # í™•ë¥ ê°’ì„ ì•Œë ¤ì¤Œ. ì´ í™•ë¥ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ 0, 1 thresholdë¥¼ ë‹¤ì‹œ ì •í•´ì¤˜ì•¼í•¨
print(result)  # [[0.564846]]   ==> í•©ê²©!!! 
```

---

## 3. `sklearn`ìœ¼ë¡œ êµ¬í˜„

`sklearn`ìœ¼ë¡œëŠ” ë§¤ìš° ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.

```python
# LogisticRegression ê°ì²´ ì„¤ì •
model = linear_model.LogisticRegression()

model.fit(x_data, t_data.ravel())

scaled_predict_data = scaler_x.transform([[600, 3.8, 1]])

result = model.predict(scaled_predict_data)
print(result)   # [1]

result_proba = model.predict_proba(scaled_predict_data)
print(result_proba)  # [[0.45454108 0.54545892]]
```

`sklearn`ì€ ì´ë ‡ê²Œ ì•½ 54.5%ì˜ í™•ë¥ ê°’ê³¼ í•¨ê»˜ í•©ê²©í•œë‹¤ê³  ì˜ˆì¸¡í•˜ì˜€ë‹¤.



ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„  ê°ê¸° ë‹¤ë¥¸ íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•´ì„œ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ì„ ì‚¬ìš©í•˜ì—¬ MLì„ êµ¬í˜„í•´ë³´ì•˜ë‹¤. 

ë°ì´í„°ê°€ ë¹ˆì•½í•œ ë°ì´í„° ì…‹ê³¼ ì˜ˆì œë¥¼ ìœ„í•´ ê±´ë„ˆë›´ ë§ì€ ë¶€ë¶„ì´ ìˆë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë‹¤ì†Œ ë¹ˆì•½í•œ ê²°ê³¼ê°€ ë‚˜ì˜¨ê±° ê°™ë‹¤.

ë‹¤ë¥¸ ì˜ˆì œì—ì„  ì–‘ì§ˆì˜ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬, ì¢€ ì™„ì„±ë„ ë†’ì€ ê²°ê´ê°’ì„ ë„ì¶œí•´ë³´ê² ë‹¤.

ê·¸ëŸ¼ ë„ì. ğŸ‘‹ 
