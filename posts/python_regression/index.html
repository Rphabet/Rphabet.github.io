<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="python 머신러닝을 위한 Regression 그리고 경사하강법 (Gradient descent)" /><meta property="og:locale" content="en" /><meta name="description" content="선형 회귀분석 (Linear Regression)" /><meta property="og:description" content="선형 회귀분석 (Linear Regression)" /><link rel="canonical" href="https://rphabet.github.io//posts/python_regression/" /><meta property="og:url" content="https://rphabet.github.io//posts/python_regression/" /><meta property="og:site_name" content="Big Ben’s Log" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-08-28T13:15:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="python 머신러닝을 위한 Regression 그리고 경사하강법 (Gradient descent)" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"선형 회귀분석 (Linear Regression)","url":"https://rphabet.github.io//posts/python_regression/","@type":"BlogPosting","headline":"python 머신러닝을 위한 Regression 그리고 경사하강법 (Gradient descent)","dateModified":"2021-08-28T13:34:15+09:00","datePublished":"2021-08-28T13:15:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://rphabet.github.io//posts/python_regression/"},"@context":"https://schema.org"}</script><title>python 머신러닝을 위한 Regression 그리고 경사하강법 (Gradient descent) | Big Ben's Log</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Big Ben's Log"><meta name="application-name" content="Big Ben's Log"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://cdn.jsdelivr.net/gh/cotes2020/chirpy-images/commons/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Big Ben's Log</a></div><div class="site-subtitle font-italic">Econometrics, Data, Economics in that order</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/rphabet" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['bangirim','gmail.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>python 머신러닝을 위한 Regression 그리고 경사하강법 (Gradient descent)</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>python 머신러닝을 위한 Regression 그리고 경사하강법 (Gradient descent)</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Girim Ban </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sat, Aug 28, 2021, 1:15 PM +0900" >Aug 28<i class="unloaded">2021-08-28T13:15:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sat, Aug 28, 2021, 1:34 PM +0900" >Aug 28<i class="unloaded">2021-08-28T13:34:15+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2851 words">15 min read</span></div></div><div class="post-content"><h2 id="선형-회귀분석-linear-regression">선형 회귀분석 (Linear Regression)</h2><p>이번 포스팅은 머신러닝의 근간이 되는 친구인 <code class="language-plaintext highlighter-rouge">Regression </code>에 대해서 간단하게 알아보자.</p><p>사실 간단하게 내용을 정리할 수가 없다. 위대한 학자 <strong>Gauss</strong>가 어떻게 <strong>Least square</strong>라는 방법을 발견했으며, 회귀분석은 왜 회귀분석이며 그래서 이게 뭐 어떤걸 나타내며, 어떤 가정하에서는 다른 Regression method를 사용해야하며 등등 내로라하는 기관의 전공 대학원 학자들도 완벽하게 숙지하지 못한 사람이 꽤 있다.</p><p>그래서, 이 포스팅에선 차, 포 다 떼고</p><ol><li>이놈이 무엇인지!<li>뭐 왜 필요한지!<li>머신러닝때 어떻게 쓰이는건지!</ol><p>에 집중해서 약간의 수학(?)처럼 보이는 수식으로 설명을 한 뒤, 바로 예제 코드로 넘어가서 “<strong>이렇게 쓰여요~</strong>” 라는 것을 알아보겠다.</p><p>우선 지도학습이 어떤 개념인지 알고 있다는 가정하에… 아래의 그림을 보자.</p><p><img data-proofer-ignore data-src="/../assets/images/regression/reg.png" alt="reg" style="zoom:110%;" /></p><p>그림 중간에 보이는 테이블을 보자.</p><p>공부 시간이 <code class="language-plaintext highlighter-rouge">x</code>에 저장되어 독립변수로 작용하고, 시험점수는 <code class="language-plaintext highlighter-rouge">t</code>라는 target (aka label)이 되어 독립변수의 값에 의해 변하는 종속변수이다.</p><p>이런 일련의 관측된 값이 어떤 데이터 셋 안에 저장이 되어있을 때, 우리는 기계를 학습시킬 수 있다. 그 학습을 통해 <code class="language-plaintext highlighter-rouge">김아무개</code>씨가 7시간을 공부했을 때 시험에서 몇점을 받을 수 있는지 예측할 수 있게 된다.</p><p>이렇게 예측을 하려면 “옛다 임의의 점수 몇점이다~” 이렇게 줄 수는 없는 노릇이다.</p><p>논리적인 근거에 기초해서 어떻게, 왜 몇점이 나왔는지 알수 있어야한다.</p><p>이런 로직을 담당해주는 부분이 <code class="language-plaintext highlighter-rouge">Regression</code>이다.</p><p>이 <code class="language-plaintext highlighter-rouge">regression</code>은 어떤 가정하에서 쓰이게 된다면 일반화 시킬 수 있는 인과관계를 규명할 수 있는 강력한 수단이다. (물론 무분별한 사용은 무의미한 결과만 도출할 뿐이다.)</p><p>자 그럼 어떻게 사용하는지 바로 알아보자.</p><p>Classic Linear Regression 이라는 아주 고전적이며 자주 쓰이는 이 회귀분석 수식의 기본적인 생김새는</p>\[\hat{y} = \beta_0 + \sum_{i=1}^{p}\beta_i x_i\]<p>이렇게 생겼다. 어째 자세히 보면 어딘가 익숙한 거 같다.. $ y = ax + b $ ! 이 단순 기울기와 절편이 있는 직선 그래프 함수와 비슷하게 생기지 않았는가?</p><p>그래서 이 친구도 직선의 속성을 갖고 있어서 선형 회귀분석이라고 불린다.</p><p><strong>Machine Learning</strong>에서는 이 직선을 표현할 때 조금 다르게 표현을 하는데</p>\[y = Wx + b\]<p>라고 표현을 한다. $ W $ 는 weight의 의미를 가진 가중치이며, $ b $는 bias라고 불린다.</p><p>이게 정말 중요하고 유의미해질 때가 있는데, 바로 실측 데이터 target <code class="language-plaintext highlighter-rouge">t</code>과 회귀분석으로 추정된 데이터 $\hat{y}$ 의 차이를 비교했을 때이다.</p><p>왜냐고? 차이의 값을 <strong>오차(error)</strong>라고 표현을 하는데, 차이의 값이 크다면 해당 직선이 데이터를 잘 표현하고 있지 못하단 소리니깐!</p><p>그래서 우리가 <strong>ML</strong>을 사용하는 대부분의 경우는 <strong>error</strong>의 값이 최소화가 되는 $ W $ 와 $ b $ 를 찾기 위함이다.</p><p>또한 <strong>error</strong>의 값을 제곱하여 합산한 형태를 우리는 쓰게 될 것이다. (왜 이런지 궁금하면 계량경제학의 회귀이론이나 통계학을 참고하기 바란다.. 너무 복잡한 설명이 될 거 같아서 이 포스팅에선 생략한다.)</p><p>이 <strong>error</strong>함수를 <strong>ML</strong>에선 <strong>손실 함수</strong> (<strong>loss function</strong>)라고 표현을 한다.</p><p>수식을 이용하여 표현을 해보자면:</p>\[E(W,b) = \frac{(t_1 - y_1)^2 + (t_2 - y_2)^2 + . . . + (t_n - y_n)^2}{n}\] \[E(W, b) = \frac{[t_1 - (Wx_1 + b)]^2 + [t_2 - (Wx_2 + b)]^2 + . . . + [t_n - (Wx_n + b)]^2} {n}\]<p>따러서, $ E(W, b) = \frac{1}{n} \sum_{i=1}^{n}[t_i - (Wx_i + b)]^2 의 형태로 나타낼 수 있다.</p><p>이 <strong>loss function</strong>은 $ W $ 와 $ b $ 의 함수이다. 왜냐고? 입력값인 $ x_i $와 레이블인 $ t_i $는 이미 정해져있기 때문이다.</p><p>또한, 식 전체에 적용된 제곱을 보면 2차 함수이며, U-모양의 컨벡스(Convex)형태를 띄고 있는 것을 알아볼 수 있을것이다.</p><p>따라서 <strong>ML</strong>에서 이 손실함수의 쓰임새는 손실함수의 값을 최소로 만들어주는 $ W $ 와 $ b $를 구하는 모델을 만드는 것이다.</p><h3 id="경사하강법-gradient-descent">경사하강법 (gradient descent)</h3><p>최적의 가중치와 바이어스 값을 찾아가는 알고리즘으로 쓰이는 <strong>경사하강법</strong>을 간략하게 설명하자면</p><ol><li>임의의 가중치 $W$ 를 선택<li>손실함수 곡선 어느 위치에서든 직선의 기울기를 구한다 (다시 말하면 해당점의 미분 계수를 찾는다. 아래의 그래프를 보자.).<li>$ W’ = W - \alpha \times \frac{\partial E(W,b)}{\partial W} $, where $ \alpha $는 <strong><em>learning rate</em></strong>라는 상수<ul><li><strong><em>learning rate</em></strong>는 곡선의 한점에서 다른 점으로 이동할 때, 값이 너무 크게 올라가는것을 방지하기 위한 수단으로 쓰인다.<li><strong><em>learning rate</em></strong>는 사용자가 임의로 customizing 해야한다. (보통은 작은 수로 설정 e.g. 0.001)</ul><li>반복횟수를 지정해줘야한다.<ul><li>이것 또한 임의로 몇번 반복해야하는지 정해줘야함.</ul></ol><p><img data-proofer-ignore data-src="/../assets/images/regression/gradient_descent.png" alt="reg" style="zoom:110%;" /></p><p>자 그러면 지금까지의 작업을 흐름에 따라 한 번 살펴보자 (밑에 손으로 그린 flow chart를 보자)</p><p><img data-proofer-ignore data-src="/../assets/images/regression/flow_chart.png" alt="reg" style="zoom:110%;" /></p><p>실측 데이터를 통해 모델을 만들고, <strong>경사하강법</strong>으로 손실함수의 최소 값을 알려주는 $W$와 $b$를 찾아낸다.</p><h3 id="실전-파이썬-코드">실전 파이썬 코드</h3><p>이제 실전에 <strong>선형 회귀 분석</strong>을 이용해보자.</p><div lang="python" class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>   <span class="c1"># 이번 예제에선 numpy만 사용할 것임
</span>
<span class="c1"># 1. training data set 준비
</span><span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]).</span><span class="n">reshape</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>    <span class="c1"># 5x1 행렬로 x(input) 값을 설정
</span><span class="n">t_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">]).</span><span class="n">reshape</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>   <span class="c1"># 5x1 행렬로 t(레이블) 값을 설정
</span>
<span class="c1"># 2. 모델 정의 (가중치 W 와 바이아스 b값을 난수로 주자)
</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 1x1 행렬로 [0,1) 사이의 값을 설정
</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 1차원 벡터로 [0, 1) 사이의 값을 설정
</span>
<span class="c1"># 3. loss function
</span><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">input_value</span><span class="p">):</span>
    <span class="s">"""
    input value는 W와 b값을 포함하고 있다
    """</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">input_value</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 원래 1x1 행렬이지만 그래도 다시 설정을 해주었음
</span>    <span class="n">b</span> <span class="o">=</span> <span class="n">input_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># model y = Wx + b
</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>   <span class="c1"># b는 브로드캐스팅 됨
</span>    <span class="c1">#   x_data  x  W   =&gt; 5x1 행렬  
</span>    <span class="c1">#    (5x1)  (1x1)  = (5x1)
</span>    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">power</span><span class="p">(</span><span class="n">t_data</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>   <span class="c1"># (실측치 - 예측치)의 제곱값의 평균
</span>  
<span class="c1"># 4. 미분을 수행할 함수
</span><span class="k">def</span> <span class="nf">numerical_derivative</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="s">"""
    편미분을 여러번 돌려야함 (vector of partial derivatives)
    f: loss function에 대해서 돌림
    """</span>
    
    <span class="n">delta_x</span> <span class="o">=</span> <span class="mf">1e-4</span>   <span class="c1"># x 변화량의 크기 (극한 대신 아주 작은 수로 대체)
</span>    <span class="n">derivative_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># x와 똑같은 shape을 가진 배열(혹은 행렬) 생성 (요소값은 0임)
</span>    
    <span class="n">it</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="p">[</span><span class="s">'multi_index'</span><span class="p">])</span> <span class="c1"># iterator 하나 생성. 플래그는 멀티 인덱스 설정
</span>    
    <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="p">.</span><span class="n">finished</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">it</span><span class="p">.</span><span class="n">multi_index</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>   <span class="c1"># 임시 변수로 x[idx] 의 원값을 저장
</span>        
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp</span> <span class="o">+</span> <span class="n">delta_x</span>  <span class="c1"># 전향 차분
</span>        <span class="n">fx_plus_delta_x</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 첫번째 인자로 들어온 함수를 대입
</span>        
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp</span> <span class="o">-</span> <span class="n">delta_x</span>  <span class="c1"># 후향 차분
</span>        <span class="n">fx_minus_delta_x</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 첫번째 인자로 들어온 함수를 다시 대입
</span>        
        <span class="c1"># 중앙 차분
</span>        <span class="n">derivative_x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fx_plus_delta_x</span> <span class="o">-</span> <span class="n">fx_minus_delta_x</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">delta_x</span><span class="p">)</span>
        
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp</span> <span class="c1"># x값 원상태로 복구
</span>        
        <span class="n">it</span><span class="p">.</span><span class="n">iternext</span><span class="p">()</span>  <span class="c1"># 다음 인덱스로 넘어가자
</span>        
    <span class="k">return</span> <span class="n">derivative_x</span>
  
<span class="c1"># 5. Learning rate 설정
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span>   <span class="c1"># 커스터마이징한 값임. 
</span>
<span class="c1"># 6. 반복 학습을 진행
</span><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">):</span>
    
    <span class="c1"># 최적의 loss값인가요? =&gt; 이거 판단하는게 쉽지 않음. 딱 0으로 떨어져서 끝나면 얼마나 좋겠어..
</span>    <span class="c1"># 넘겨줄 파라미터부터 구하자  =&gt; ravel() 이용하자. 뭐든지 1차원으로 바꿈
</span>    <span class="n">input_param</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">W</span><span class="p">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">b</span><span class="p">.</span><span class="n">ravel</span><span class="p">()),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 열방향 =&gt; [W, b] 이렇게 들어가게 됨
</span>    <span class="n">tmp</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">numerical_derivative</span><span class="p">(</span><span class="n">loss_func</span><span class="p">,</span> <span class="n">input_param</span><span class="p">)</span>   <span class="c1"># [W의 편미분 값, b의 편미분 값]
</span>    
    <span class="n">W</span> <span class="o">=</span> <span class="n">W</span> <span class="o">-</span> <span class="n">tmp</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># 얘도 broadcasting 되긴 하는데 reshape 잡자.
</span>    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">tmp</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>   <span class="c1"># broadcasting 
</span>    
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>   <span class="c1"># 10만번의 학습이 진행되는 동안, 중간 과정 10번 체크
</span>        <span class="k">print</span><span class="p">(</span><span class="s">'W의 값은 {}, b의 값은 {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
        

</pre></table></code></div></div><p>나같은 경우엔 이렇게 실행을 시키면…</p><div lang="python" class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="n">W의</span> <span class="n">값은</span> <span class="p">[[</span><span class="mf">0.68000733</span><span class="p">]],</span> <span class="n">b의</span> <span class="n">값은</span> <span class="p">[</span><span class="mf">0.97612513</span><span class="p">]</span>
<span class="n">W의</span> <span class="n">값은</span> <span class="p">[[</span><span class="mf">1.93730769</span><span class="p">]],</span> <span class="n">b의</span> <span class="n">값은</span> <span class="p">[</span><span class="mf">1.22633913</span><span class="p">]</span>
<span class="n">W의</span> <span class="n">값은</span> <span class="p">[[</span><span class="mf">1.9552927</span><span class="p">]],</span> <span class="n">b의</span> <span class="n">값은</span> <span class="p">[</span><span class="mf">1.16140755</span><span class="p">]</span>
<span class="n">W의</span> <span class="n">값은</span> <span class="p">[[</span><span class="mf">1.96811821</span><span class="p">]],</span> <span class="n">b의</span> <span class="n">값은</span> <span class="p">[</span><span class="mf">1.11510337</span><span class="p">]</span>
<span class="n">W의</span> <span class="n">값은</span> <span class="p">[[</span><span class="mf">1.97726438</span><span class="p">]],</span> <span class="n">b의</span> <span class="n">값은</span> <span class="p">[</span><span class="mf">1.08208281</span><span class="p">]</span>
<span class="n">W의</span> <span class="n">값은</span> <span class="p">[[</span><span class="mf">1.98378671</span><span class="p">]],</span> <span class="n">b의</span> <span class="n">값은</span> <span class="p">[</span><span class="mf">1.05853511</span><span class="p">]</span>
<span class="n">W의</span> <span class="n">값은</span> <span class="p">[[</span><span class="mf">1.98843794</span><span class="p">]],</span> <span class="n">b의</span> <span class="n">값은</span> <span class="p">[</span><span class="mf">1.04174271</span><span class="p">]</span>
<span class="n">W의</span> <span class="n">값은</span> <span class="p">[[</span><span class="mf">1.99175483</span><span class="p">]],</span> <span class="n">b의</span> <span class="n">값은</span> <span class="p">[</span><span class="mf">1.02976767</span><span class="p">]</span>
<span class="n">W의</span> <span class="n">값은</span> <span class="p">[[</span><span class="mf">1.99412018</span><span class="p">]],</span> <span class="n">b의</span> <span class="n">값은</span> <span class="p">[</span><span class="mf">1.021228</span><span class="p">]</span>
<span class="n">W의</span> <span class="n">값은</span> <span class="p">[[</span><span class="mf">1.99580697</span><span class="p">]],</span> <span class="n">b의</span> <span class="n">값은</span> <span class="p">[</span><span class="mf">1.01513817</span><span class="p">]</span>

<span class="n">이렇게</span> <span class="n">값이</span> <span class="n">나왔다</span><span class="err">!</span> 
<span class="n">물론</span> <span class="n">seed를</span> <span class="n">잡고</span> <span class="n">실행하면</span> <span class="n">난수에</span> <span class="n">관해서</span> <span class="n">고정된</span> <span class="n">값이</span> <span class="n">출력이</span> <span class="n">되기에</span><span class="p">,</span> <span class="n">더</span> <span class="n">편할</span> <span class="n">수도</span> <span class="n">있다</span><span class="p">.</span> <span class="n">참고하자</span><span class="p">.</span>
</pre></table></code></div></div><p>자 그럼 $ W $와 $b$의 값을 갖고 뭘 해야하겠는가??</p><p><code class="language-plaintext highlighter-rouge">y = Wx + b</code>를 예측하기 딱 좋지 않는가?</p><p>임의의 값 <code class="language-plaintext highlighter-rouge">x</code>를 설정하자. 음… 7이란 값을 주겠다.</p><div lang="python" class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="n">predict_value</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">7</span><span class="p">]])</span>   
<span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">predict_value</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>  <span class="c1"># hypothesis model     X = predict_value
</span>
<span class="k">print</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
<span class="s">"""
결과값: [[14.98986402]]

X가 공부시간이고, y가 시험 성적이라고 가정을 해본다면
"7시간 공부했을때 약 15점에 가까운 성적을 받을 수 있으리라 예측된다." 라고 해석할 수 있겠다.
물론 이 값은

x_data = np.arange([1, 2, 3, 4, 5]).reshape(5, 1)    # 5x1 행렬로 x(input) 값을 설정
t_data = np.arange([3, 5, 7, 9, 11]).reshape(5, 1)   # 5x1 행렬로 t(레이블) 값을 설정

이 training data set에 의거한 결과임을 잊지 말자.
"""</span>
</pre></table></code></div></div><p>이렇게 어떻게 <code class="language-plaintext highlighter-rouge">Linear Regression</code>기법이 머신러닝에서 사용되는지 경사하강법을 통해 알아보았다.</p><p>그럼 안녕 👋</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/python/'>python</a>, <a href='/categories/regression/'>regression</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/python/" class="post-tag no-text-decoration" >python</a> <a href="/tags/ml/" class="post-tag no-text-decoration" >ML</a> <a href="/tags/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/" class="post-tag no-text-decoration" >머신러닝</a> <a href="/tags/%EB%AF%B8%EB%B6%84/" class="post-tag no-text-decoration" >미분</a> <a href="/tags/%ED%8C%8C%EC%9D%B4%EC%8D%AC/" class="post-tag no-text-decoration" >파이썬</a> <a href="/tags/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/" class="post-tag no-text-decoration" >회귀분석</a> <a href="/tags/regression/" class="post-tag no-text-decoration" >regression</a> <a href="/tags/mse/" class="post-tag no-text-decoration" >MSE</a> <a href="/tags/ols/" class="post-tag no-text-decoration" >OLS</a> <a href="/tags/gradient-descent/" class="post-tag no-text-decoration" >gradient descent</a> <a href="/tags/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95/" class="post-tag no-text-decoration" >경사하강법</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=python 머신러닝을 위한 Regression 그리고 경사하강법 (Gradient descent) - Big Ben's Log&url=https://rphabet.github.io//posts/python_regression/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=python 머신러닝을 위한 Regression 그리고 경사하강법 (Gradient descent) - Big Ben's Log&u=https://rphabet.github.io//posts/python_regression/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=python 머신러닝을 위한 Regression 그리고 경사하강법 (Gradient descent) - Big Ben's Log&url=https://rphabet.github.io//posts/python_regression/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink('', 'Link copied successfully!')" data-toggle="tooltip" data-placement="top" title="Copy link"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/multinomial/">머신러닝 다중분류 로지스틱 작동 원리 Multinomial Classification</a><li><a href="/posts/2021-09-08-Logistic_cancer_example_tf/">python으로 하는 머신러닝 로지스틱 회귀분석 예제 (tensorflow version)</a><li><a href="/posts/Logistic_cancer_example/">python으로 하는 머신러닝 로지스틱 회귀분석 예제 (scikit learn version)</a><li><a href="/posts/TensorFlow_Fundamentals/">TensorFlow 기초</a><li><a href="/posts/TensorFlow_multiple_regression/">TensorFlow를 이용한 다중선형회귀분석</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/ml/">ML</a> <a class="post-tag" href="/tags/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/">머신러닝</a> <a class="post-tag" href="/tags/numpy/">numpy</a> <a class="post-tag" href="/tags/pandas/">pandas</a> <a class="post-tag" href="/tags/regression/">regression</a> <a class="post-tag" href="/tags/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/">회귀분석</a> <a class="post-tag" href="/tags/tensorflow/">tensorflow</a> <a class="post-tag" href="/tags/%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C%EC%9A%B0/">텐서플로우</a> <a class="post-tag" href="/tags/logistic/">logistic</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/ML_reg_data_handling/"><div class="card-body"> <span class="timeago small" >Sep 1<i class="unloaded">2021-09-01T16:57:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>python으로 하는 머신러닝 선형회귀분석 이상치, 결치 처리 그리고 정규화</h3><div class="text-muted small"><p> Linear Regression Data Handling 이번 포스팅은 파이썬과 경사하강법 (Gradient Descent Algorithm)을 이용하여 지난번보다 더 정확한 회귀분석을 해보려고 한다. 그리고 최종적으로 내가 짠 코드와 알고리즘이 scikit learn 패키지에서 제공한 linear_model에 모듈에 비해서 얼마나 정확한지 비교...</p></div></div></a></div><div class="card"> <a href="/posts/python_logistic_regression/"><div class="card-body"> <span class="timeago small" >Sep 5<i class="unloaded">2021-09-05T16:30:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>python으로 하는 머신러닝 로지스틱 회귀분석 개념</h3><div class="text-muted small"><p> Logistic Regression 지난번 포스팅에선 Machine Learning 으로 어떻게 회귀분석을 하는지 알아보았다. 오늘은 레이블(t) 값이 0과 1 둘중 하나인 이산(discrete) 형태를 띄고 있을 때 어떻게 회귀분석을 할 수 있는지 알아보자. 사실 학계에서는 이런 이산변수를 다루는 일보다 연속적인 값의 형태를 가진 종속변수를...</p></div></div></a></div><div class="card"> <a href="/posts/python_logistic_reg_with_code/"><div class="card-body"> <span class="timeago small" >Sep 5<i class="unloaded">2021-09-05T18:34:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>python으로 하는 머신러닝 로지스틱 회귀분석</h3><div class="text-muted small"><p> Logistic Regression 이번 포스팅에선 로지스틱 회귀분석을 python으로, tensorflow로, 그리고 sklearn으로 직접 구현해보자. 공부시간과 시험 합격에 대한 데이터를 간단하게 직접 만들어서 사용하자. 1. python으로 하는 logistic regression 1 2 3 4 5 6 7 8 9 10 11 12 13...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/python_partial_differentiation/" class="btn btn-outline-primary" prompt="Older"><p>python으로 하는 수치미분 (Numerical Differentiation) (2)</p></a> <a href="/posts/ML_reg_data_handling/" class="btn btn-outline-primary" prompt="Newer"><p>python으로 하는 머신러닝 선형회귀분석 이상치, 결치 처리 그리고 정규화</p></a></div><div id="disqus" class="pt-2 pb-2"><p class="text-center text-muted small pb-5"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script src="/assets/js/lib/jquery.disqusloader.min.js"></script> <script> const options = { scriptUrl: '//rphabet-github-io.disqus.com/embed.js', disqusConfig: function() { this.page.title = 'python 머신러닝을 위한 Regression 그리고 경사하강법 (Gradient descent)'; this.page.url = 'https://rphabet.github.io//posts/python_regression/'; this.page.identifier = '/posts/python_regression/'; } }; $.disqusLoader('#disqus', options); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://twitter.com/username">Girim Ban</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/ml/">ML</a> <a class="post-tag" href="/tags/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/">머신러닝</a> <a class="post-tag" href="/tags/numpy/">numpy</a> <a class="post-tag" href="/tags/pandas/">pandas</a> <a class="post-tag" href="/tags/regression/">regression</a> <a class="post-tag" href="/tags/%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/">회귀분석</a> <a class="post-tag" href="/tags/tensorflow/">tensorflow</a> <a class="post-tag" href="/tags/%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C%EC%9A%B0/">텐서플로우</a> <a class="post-tag" href="/tags/logistic/">logistic</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://rphabet.github.io/{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-8TH4RLJ51D"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-8TH4RLJ51D'); }); </script>
